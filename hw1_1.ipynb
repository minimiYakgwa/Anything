{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10cb82d7-42c1-4c4d-89f3-b08d02eccb58",
   "metadata": {},
   "source": [
    "> # **a_tensor_initialization.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1caf6987-580e-4892-b642-e50fafe98855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:14:51.069485Z",
     "start_time": "2024-09-18T11:14:37.889349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 105\u001b[0m\n\u001b[0;32m     97\u001b[0m a10 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([                 \u001b[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     99\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m    100\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m    101\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m    102\u001b[0m ])\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(a10\u001b[38;5;241m.\u001b[39mshape, a10\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m--> 105\u001b[0m a11 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;49;00m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 4차원 size가 일정해야 하는데, 한쪽은 2, 한쪽은 3이라 오류 발생함\u001b[39;49;00m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1 ,2 ,3], device='cpu') # 데이터 타입이 float64로만 선언되는 tensor형 변수 t1 선언하기. 그리고 tensor를 cpu에 올리기\n",
    "print(t1.dtype) # 변수 t1의 데이터 타입 출력하기\n",
    "print(t1.device) # 변수 t1이 어디에 올려져 있는지 출력\n",
    "print(t1.requires_grad) # tensor의 기울기 저장 여부 출력. 기울기 사용처는 자동 미분이 있음\n",
    "print(t1.size()) # tensor의 크기 출력\n",
    "print(t1.shape) # tensor의 크기 출력 ( t1.size() == t1.shape )\n",
    "\n",
    "\"\"\"\n",
    "t100 = torch.tensor([1, 2, '3'])\n",
    "print(t100)                      # tensor는 반드시 한 타입의 데이터로만 정의되므로 error.\n",
    "a100 = torch.tensor(['a', 'b'])\n",
    "print(a100)                      # tensor는 숫자 데이터만 들어갈 수 있으므로 error\n",
    "\"\"\"\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda')) - t1_cuda를 gpu 'cuda'에 올려놓음\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu') # 입력된 데이터에 따라 타입이 변하는 tensor형 변수 t2선언하기. 그리고 cpu에 올리기\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    " \n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0 \n",
    "print(a1.shape, a1.ndim)             # ndim : tensor가 몇차원 데이터를 저장하고 있는지 출력함\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],           # 4차원 size가 일정해야 하는데, 한쪽은 2, 한쪽은 3이라 오류 발생함\n",
    "    [[[1, 2, 3], [4, 5]]],    \n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "\n",
    "\n",
    "#한쪽 차원을 축소시키거나, 한쪽 차원을 추가시키면 오류가 해결됨.\n",
    "# a11 = torch.tensor([\n",
    "#     [[[1, 2, 3], [4, 5, 6]]],           \n",
    "#     [[[1, 2, 3], [4, 5, 6]]],\n",
    "#     [[[1, 2, 3], [4, 5, 6]]],\n",
    "#     [[[1, 2, 3], [4, 5, 6]]],\n",
    "# ])\n",
    "# print(a11.shape, a11.ndim) : 에러 없음\n",
    "\n",
    "# a12 = torch.tensor([                \n",
    "#     [[[1, 2], [4, 5]]],           \n",
    "#     [[[1, 2], [4, 5]]],\n",
    "#     [[[1, 2], [4, 5]]],\n",
    "#     [[[1, 2], [4, 5]]],\n",
    "# ])\n",
    "# print(a12.shape, a12.ndim) : 에러 없음\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04d531-745b-4f07-a087-fecf90ea4cc6",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> tensor는 문자 데이터를 담지 못하므로 문자 데이터를 아스키코드 등으로 정수화 시킨 후 담을 수 있을 것 같음. <br>\n",
    "> dict 타입을 텐서화 시키려면 데이터 값만 tensor화 한 상태로 새로운 dict을 만들면 된다는 것을 찾아서 알 수 있었음. <br>\n",
    "> tensor는 차원의 size를 정확히 지켜야 한다는 것을 알게 되었음. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cecdf-a890-41e5-beb5-090d3fb4e3e2",
   "metadata": {},
   "source": [
    "> # **b_tensor_initialization_copy.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "50fb602a-ab7d-4bdd-87fe-cd3f6a9704cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T08:45:25.828298Z",
     "start_time": "2024-09-22T08:45:17.760484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1) # 실수형 타입 tensor로 변환하기 때문에 안의 데이터도 실수형으로 변환됨\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2) # 입력한 값에 따라 데이터의 타입이 바뀜.\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) # torch.from_numpy()와 동일. 입력한 값에 따라 데이터 타입이 변경됨.\n",
    "\n",
    "\n",
    "# torch.from_numpy 역시 torch.as_tensor와 동일하게 작동하는 것을 알 수 있음.\n",
    "# l100 = np.array([1, 3, 5])\n",
    "# print(l100)\n",
    "# t100 = torch.from_numpy(l100)\n",
    "# print(t100)\n",
    "# l100[0] = 0\n",
    "# print(l100)\n",
    "# print(t100)\n",
    "\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3) # 리스트 형태의 데이터를 이용하여 tensor를 생성했을 경우 이전 데이터를 참조, 생성방법과 상관없이 이전 데이터를 변경하였더라도 tensor가 변경되지 않음.\n",
    "\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6) # ndarray형태의 데이터를 이용하여 tensor를 생성했을 경우, torch.as_tensor / torch.from_numpy를 사용하였다면 이전 데이터와 공유하게 되며,\n",
    "          # 이를 변경하였을 때 tensor 또한 변경됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4c4a-cbd6-46bb-9dc7-cdb1ef56de48",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> ndarray 형태를 as_tensor / from_numpy를 사용하여 tensor화 하면 참조한다고 알게 되었는데, 참조를 통한 tensor화를 사용할까 고민하게 되었음. 데이터의 양이 매우 많기 때문에 복사를 통한 tensor생성보단 참조가 더 효율적이라고 생각하게 되었음.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6a82d-f5f5-4f9d-a498-9d483a038d72",
   "metadata": {},
   "source": [
    "> # **c_tensor_initialization_constant_values.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b58d8bd-7c6c-49d0-a503-6cb4ee9264e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0000, 1.8750, 5.0000, 5.0000])\n",
      "tensor([0.0000, 1.8750, 3.0000, 4.0000])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5) 5 크기의 1로 초기화된 1차원 tensor 생성 ( 실수 값으로 초기화함 )\n",
    "t1_like = torch.ones_like(input=t1) # t1와 동일한 크기의 1로 초기화된 1차원 tensor 생성\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "\n",
    "# 여러 차원의 tensor 제작에도 사용할 수 있다는 것을 알 수 있음.\n",
    "# t100 = torch.ones(size=(3,2))  \n",
    "# print(t100)\n",
    "\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6) 6 크기의 0으로 초기화된 1차원 tensor 생성\n",
    "t2_like = torch.zeros_like(input=t2) # t2와 동일한 크기의 0으로 초기화된 1차원 tensor 생성\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4) 4 크기의 빈 공간으로 초기화된 1차원 tensor 생성\n",
    "t3_like = torch.empty_like(input=t3) # t3와 동일한 크기의 빈 공간으로 초기화된 1차원 tensor 생성\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3) # 3 크기의 대각성분만 1인 2차원 tensor 생성\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd915e0-a7b4-4546-9ecb-cfc8448e65dc",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> 여러 크기의 tensor에 대해서 적용할 수 있다는 것을 알 수 있었음.<br>\n",
    "> empty는 빈 공간으로 초기화하기 때문에, 저장공간에 남아있는 데이터가 출력될 수 있다는 것을 알 수 있었음.<br>\n",
    "> 그래서 매번 실행할 때마다 값이 다르게 나왔음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4685a-1b4b-4be4-b965-0dbb88d8b798",
   "metadata": {},
   "source": [
    "> # **d_tensor_initialization_random_values.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "211ce4f1-3d76-4aa7-9cb5-fb7984f29359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T12:45:24.716408Z",
     "start_time": "2024-09-11T12:45:17.846886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 12]])\n",
      "tensor([[0.0453, 0.5035, 0.9978]])\n",
      "tensor([[0.7419, 0.5923, 0.2908]])\n",
      "tensor([[ 9.2270, 10.0961],\n",
      "        [ 9.4067,  9.5878],\n",
      "        [10.0763, 11.1161]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))  # low~high 사이의 정수값을 동일한 확률로 랜덤 생성해서 size 크기만한 tensor 생성\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))                      # 0~1사이의 실수를 동일한 확률로 랜덤으로 생성하여 size 크기만한 tensor 생성\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))                     # 분산이 1이고 평균이 0인 표준정규분포 값 중에 랜덤으로 생성해서 tensor 생성\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))# 평균과 표준편차가 주어진 정규분포에서 값을 랜덤으로 생성해서 tensor 생성\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)  # 시작값과 종료값 사이에 일정한 간격으로 구성된 값들을 step값 만큼 생성하여 tensor 생성\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)                              # 0~4사이의 정수가 순서대로 배정된 tensor 생성. range 개념과 유사함.\n",
    "print(t6)\n",
    "\n",
    "\n",
    "# t100 = torch.randperm(7)                           -> tensor([6, 3, 0, 2, 5, 1, 4]) 0~입력값까지의 정수값을 한번씩 나오게 랜덤으로 생성\n",
    "\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)                           # random seed(동일한 난수)를 고정. 같은 결과가 나오게 만들어줌.\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06125620-25ad-4f15-847a-3b1536b58c9f",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> torch.randperm()도 랜덤값을 출력하는 함수라는 것을 알게 되었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4757fb3-d40c-44cc-8cbd-9a22c9d775d3",
   "metadata": {},
   "source": [
    "> # **e_tensor_type_conversion.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "477af26c-a2fc-422f-a52b-9e9c6d87af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16) # tensor의 데이터타입을 임의로 변경함.\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20. # *torch.rand() * a는 0~a 범위 중 랜덤으로 값을 추출함.\n",
    "print(c)\n",
    "\n",
    "\n",
    "# *a는 여러 랜덤함수에도 적용됨.\n",
    "# t1000 = torch.randint(low=0, high=10, size=(3, 3)) * 2\n",
    "# print(t1000)\n",
    "# t1001 = torch.randn(size=(3, 3)) * 2                  \n",
    "# print(t1001)\n",
    "# t1002 = torch.linspace(start=1.0, end=10.0, steps=5) * 2\n",
    "# print(t1002)\n",
    "\n",
    "d = b.to(torch.int32) # tensor의 데이터타입을 int16 -> int32로 변경함.\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double) # tensor의 데이터타입을 실수로 바꿈\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short) # tensor의 데이터타입을 정수로 바꿈\n",
    "\n",
    "double_d = torch.zeros(10, 2).double() # tensor의 데이터타입을 실수로 바꿈\n",
    "short_e = torch.ones(10, 2).short() # tensor의 데이터타입을 정수로 바꿈\n",
    "\n",
    "\n",
    "#형변환 함수는 대각행렬함수에 적용할 수 있는지 확인.\n",
    "# qq = torch.eye(3).short()\n",
    "# print(qq)\n",
    "# qq = qq.to(torch.float64)\n",
    "# print(qq)\n",
    "# qq = torch.eye(3, dtype=torch.int16)\n",
    "# print(qq)\n",
    "\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double) # tensor의 데이터타입을 실수로 바꿈\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short) # tensor의 데이터타입을 정수로 바꿈\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double) # tensor의 데이터타입을 실수로 바꿈\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short) # tensor의 데이터타입을 정수로 바꿈\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b6c4a-792f-4d82-9b26-bc324f61cced",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> 형변환은 대부분 잘 적용되는 것을 알 수 있었고, '*값'도 역시 대부분 잘 적용된다는 것을 알 수 있었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7475e7-86d0-493a-a5dc-ac16377d9502",
   "metadata": {},
   "source": [
    "> # **f.tensor_operations.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2aa68e9a-8e14-4ed0-9918-5e8215cec45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise operations는 각 요소에 대해 연산하는 것을 의미하며, add(), sub(), mul(), div() 등이 있음.\n",
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2) # torch.add(), \"+\"를 통해 같은 위치의 데이터를 서로 더해줌.\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "\n",
    "# t100 = torch.ones(3, 3)\n",
    "# t101 = torch.ones_like(t100)\n",
    "# print(t100 % t101)            : %연산에 대해서도 잘 적용됨\n",
    "# print(t100 // t101)           : //연산에 대해서도 잘 적용됨\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t5 = torch.sub(t1, t2) # torch.sub(), '-'연산자를 통해 같은 위치의 데이터를 서로 빼줌.\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t7 = torch.mul(t1, t2) # torch.mul(), '*' 연산자를 통해 같은 위치의 데이터를 서로 곱해줌.\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t9 = torch.div(t1, t2) # torch.div(), '/'연산자를 통해 같은 위치의 데이터를 서로 나눠줌.\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd4597-04d4-4463-9ff4-800578968a5f",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> 몫 연산, 나머지 연산에 대해서도 element-wise operations이 잘 적용된다는 것을 알 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc870ee-9870-4a39-a234-35e863e509d7",
   "metadata": {},
   "source": [
    "> # **g_tensor_operations_mm.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c8ceeff8-c19f-4e1e-aa52-4b9305f010da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1]) # 두 텐서의 내적?값이 출력됨. size는 torch.tensor(3)처럼 빈 공간이라고 출력됨.\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "\n",
    "#torch.dot()은 1차원 tensor만 가능\n",
    "# t100 = torch.dot(torch.tensor([[1,2], [3, 4]]), torch.tensor([[5,2], [3,3]])) -> error \n",
    "# print(t100, t100.size())\n",
    "\n",
    "\n",
    "t2 = torch.randn(2, 3) \n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3) # 2차원 행렬곱 수행. 단, 두 행렬의 size가 맞아야함.\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6) # 3차원 행렬곱 수행. 첫번째 차원을 제외하고 행렬곱 수행. 단, 첫 행렬(batch)을 제외한 두 행렬의 size가 맞아야함.\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a5668-72f4-40d6-872c-a052e6fb966f",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa8e00-9f82-49ef-a6ad-78865e02be52",
   "metadata": {},
   "source": [
    "> # **h_tensor_operations_matmul.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "95cefefa-6ef5-44d4-b8ce-312e3a2f7e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.matmul()은 데이터의 종류에 따라 dot, mm, bmm 처럼 이용할 수 있고, broadcasting을 지원함.\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([]) \n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a49a88-ac86-43bb-81e0-70b979780caf",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050188a-f700-4aee-889b-d0a3643aa47e",
   "metadata": {},
   "source": [
    "> # **i_tensor_broadcasting.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3119edc9-a4f6-4469-83e0-43922acee44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T11:38:53.726150Z",
     "start_time": "2024-09-17T11:38:45.624158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0]) # [1.0, 2.0, 3.0] * [2.0, 2.0, 2.0]\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])# t3.size() = (3, 2) / t4.size() = (  2)\n",
    "t4 = torch.tensor([4, 5])                    # [[0,1], [2, 4], [10, 10]] - [[4, 5], [4, 5], [4, 5]]\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])     # t5.size() = (2, 2)\n",
    "print(t5 + 2.0)  # t5.add(2.0)              # [[1., 2.], [3., 4.]] + [[2.0, 2.0], [2.0, 2.0]]\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "\n",
    "# t100 = torch.ones(3, 3)\n",
    "# t101 = torch.ones(1, 1)\n",
    "# print(t101 % t100)                          : %연산, //연산에 대해서도 broadcasting이 잘 적용됨.\n",
    "# t00 = torch.ones(3, 3)\n",
    "# t01 = torch.ones(1, 1)\n",
    "# print(t00 // t01)\n",
    "\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "def normalize(x):                           # 데이터들의 범위를 한정시키기 위해 사용됨. 가령 0~256범위의 값을 가지는 이미지 데이터값을 0~1으로 변경하기 위해 사용\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)   # torch.pow()는 입력한 tensor값의 제곱된 값을 반환하게 됨.\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ec9f8-4afd-4a74-915e-bf39bf28fbd3",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> 파이썬에서 사용가능한 모든 연산에 대해 broadcasting이 잘 적용된다는 것을 알 수 있었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2d9f9-9ff9-4d7e-a3c6-5969e1466e08",
   "metadata": {},
   "source": [
    "> # **j_tensor_indexing_slicing.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d6f9f0f5-23a9-4fe6-945f-858c145f5e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "\n",
    "# t01 = torch.as_tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# print(t01)\n",
    "# print(t01[:, ::2])                                     : 일정한 간격으로 slicing 할 수 있음.\n",
    "\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)\n",
    "\n",
    "# tensor도 python list처럼 인덱싱, 슬라이싱이 동일하나는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d231fe-6fba-4839-868d-26680b262538",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> 일정한 간격으로 slicing하는 것도 가능하다는 것을 알 수 있었음. <br>\n",
    "> slicing 기능을 잘 사용하는 것이 더 좋은 모듈을 만드는 키라고 생각함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e988e0b-e1f9-433d-9c2c-632f2959e74d",
   "metadata": {},
   "source": [
    "> # **k_tensor_reshaping.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "58c6ff4e-fc7d-4c34-8322-9d5a79b3d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) #view()는 contiguous인 경우에 가능하며, reshape()는 contiguous인 경우 이전 데이터를 참조함.\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "# print(t1.contiguous) # -> true\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,) #squeeze()는 차원을 줄임. copy 형식\n",
    "\n",
    " \n",
    "# t106 = torch.ones(1, 3, 2)\n",
    "# t107 = t106.squeeze() # -> torch.Size(3, 2) : 차원 size가 1인 차원만 줄임.\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "\n",
    "# t108 = t106.squeeze(1) # -> torch.Size(1, 3, 2)\n",
    "\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1) #unsqueeze()는 차원을 늘림. copy 형식\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,) #flatten()은 다차원을 1차원으로 변형. 한쌍을 제외한 모든  괄호를 제거한다고 해석할 수 있음.\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1) #start_dim값에 해당하는 차원부터 1차원으로 변형해줌.\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5]) : 차원은 유지하면서, size를 변형하는 함수.\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2) transpose()는 행과 열을 cross해줌.\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d8581-0eb2-4358-9f49-154d407c3cfd",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> squeeze, unsqueeze는 size가 1인 차원만 줄이며, 1이 아닌 차원을 지정해도 줄이지 못한다는 것을 알 수 있었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d28e6-b858-4a7d-8789-2362db840ac8",
   "metadata": {},
   "source": [
    "> # **l_tensor_concat.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9ffb5581-f115-41b6-9dfe-ebdb98faf9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1) #concat() / cat()은 특정 차원을 stack구조처럼 쌓아주는 함수임. 단 쌓으려는 차원을 제외한 나머지 차원은 같아야 함.\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9a57b-3ebe-4af8-a887-36b3fd89b380",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> cat은 데이터 샘플의 수를 늘릴 때 주로 사용한다는 것을 알게 되었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cc1e8-f681-4afa-a9dd-b61ab3ccd138",
   "metadata": {},
   "source": [
    "> # **m_tensor_stacking.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6ce71d08-f5ee-4bb7-9f04-15713e3f35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0) #cat()과 달리 새로운 차원을 생성해 쌓아주는 형식임. stack = unsqueeze + cat\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "\n",
    "# t101 = torch.tensor([1, 2, 3])\n",
    "# t102 = torch.ones(1)\n",
    "# t103 = torch.stack((t101, t102), dim=0) -> stack 역시 나머지 차원의 크기가 같아야함.\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06ed3b-e4ba-4ea3-8066-9b4f21750d4c",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> stack은 batch 처리에 자주 사용된다는 것을 알게 되었음. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275d80c-173b-43a8-b7f9-05470d6472cb",
   "metadata": {},
   "source": [
    "> # **n_tensor_vstack_hstack.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "95e59eb0-0a56-40b9-a24b-0ebcfe0b31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))  # vstack은 수직으로 쌓음.\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11)) # hstack은 수평으로 쌓음.\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "# cat VS vstack / cat VS hstack\n",
    "# t009 = torch.tensor([[[1], [2], [3]]])\n",
    "# t0010 = t009 +3\n",
    "\n",
    "# t0011 = torch.hstack((t009, t0010))\n",
    "# t0012 = torch.cat((t009, t0010), dim=1)\n",
    "# print(t0011.equal(t0012))                  -> True\n",
    "\n",
    "# t0013 = torch.vstack((t009, t0010))\n",
    "# t0014 = torch.cat((t009, t0010), dim=0)\n",
    "# print(t0013.equal(t0014))                  -> True\n",
    "\n",
    "# print(torch.cat((t009, t0010), dim=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f7d42-f618-446b-bda3-537a62204eaf",
   "metadata": {},
   "source": [
    "> # [고찰]\n",
    "> vstack, hstack은 cat과 유사해 2차원 tensor까진 대체할 수 있지만, 3차원이상부턴 cat을 사용하지 않으면 불편하기 때문에 cat을 더 자주 쓸 것 같다고 생각함. 또한 딥러닝은 데이터가 주로 3차원이 많이 보이기 때문에 딥러닝 모델 학습에선 cat을 좀 더 쓸 것 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce0d41-f7c7-4d25-ab3d-90654c527133",
   "metadata": {},
   "source": [
    "> # **숙제 후기**\n",
    "> 텐서를 공부하면서 데이터 전처리 함수가 잘 적용되는 array라고 생각하게 되었음. 그래서 array를 잘 사용해봐서 학습하는데 큰 어려움은 없었음. 어려웠던 점은 2차원 배열과 3차원 이상의 배열의 시각적인 차이점이 괄호밖에 없다는 것이었음. 직관적으로 한번에 \"이건 몇차원이네\"라고 바로 생각하기 어려웠고, stack 결과물을 예상하는데 시간이 걸렸음. 그래서 이 어려움을 해결하는게 딥러닝 공부에 큰 관건이 될 것이라고 생각함.\n",
    "> 가장 기억에 남는 함수는 unsqueeze / squeeze인것 같음. 뒤에 나온 bike_sharing_data를 보면서 약 17000개의 데이터를 unsqueeze함수 하나로 간단하게 전처리 한게 눈에 익었음. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
